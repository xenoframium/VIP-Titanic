{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avnit\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8305084745762712\n"
     ]
    }
   ],
   "source": [
    "# Pandas is a nice utilitiy that enables some easy data manipulation, especially from a csv\n",
    "import pandas as pd\n",
    "# Numpy lets us work with arrays\n",
    "import numpy as np\n",
    "import re\n",
    "# Sklearn provides various modules with a common API\n",
    "from sklearn import svm, tree, neighbors, neural_network, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "train_data.columns[train_data.isna().any()].tolist()\n",
    "\n",
    "train_data.set_index(keys=['PassengerId'], drop=True, inplace=True)\n",
    "\n",
    "test_data.set_index(keys=['PassengerId'], drop=True, inplace=True)\n",
    "\n",
    "train_nan_map = {'Fare': train_data['Fare'].mean(), 'Embarked': train_data['Embarked'].mode()[0]}\n",
    "test_nan_map = {'Fare': test_data['Fare'].mean(), 'Embarked': test_data['Embarked'].mode()[0]}\n",
    "\n",
    "train_data.fillna(value=train_nan_map, inplace=True)\n",
    "test_data.fillna(value=test_nan_map, inplace=True)\n",
    "\n",
    "#Southampton Cherbourg Queenstown\n",
    "\n",
    "columns_map = {'Sex': {'male': 0, 'female': 1}}\n",
    "\n",
    "train_data.replace(columns_map, inplace=True)\n",
    "test_data.replace(columns_map, inplace=True)\n",
    "\n",
    "titles = set()\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\ttitles.add(title)\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\ttitles.add(title)\n",
    "\n",
    "titles.add(\"C\")\n",
    "titles.add(\"Q\")\n",
    "titles.add(\"S\")\n",
    "new_cols = dict()\n",
    "for title in titles:\n",
    "\tnew_cols[title] = [0 for x in range(0, train_data.shape[0])]\n",
    "\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "\tnew_cols[row['Embarked']][index - 1] = 1\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\tnew_cols[title][index - 1] = 1\n",
    "\n",
    "for column in new_cols.keys():\n",
    "\ttrain_data[column] = new_cols[column]\n",
    "\n",
    "new_cols = dict()\n",
    "for title in titles:\n",
    "\tnew_cols[title] = [0 for x in range(0, test_data.shape[0])]\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "\tnew_cols[row['Embarked']][index - 1 - train_data.shape[0]] = 1\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\tnew_cols[title][index - 1 - train_data.shape[0]] = 1\n",
    "\n",
    "for column in new_cols.keys():\n",
    "\ttest_data[column] = new_cols[column]\n",
    "\n",
    "both = train_data.append(test_data)\n",
    "\n",
    "title_age_sum = dict()\n",
    "title_count = dict()\n",
    "\n",
    "for title in titles:\n",
    "\ttitle_age_sum[title] = 0\n",
    "\ttitle_count[title] = 0\n",
    "\n",
    "for index, row in both.iterrows():\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\tif np.isnan(row['Age']):\n",
    "\t\tcontinue\n",
    "\ttitle_count[title] += 1\n",
    "\ttitle_age_sum[title] += row['Age']\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\tif not np.isnan(row['Age']):\n",
    "\t\tcontinue\n",
    "\ttest_data.at[index, 'Age'] = title_age_sum[title] / title_count[title]\n",
    "\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "\tm = re.search(\", ([^\\\\.]*)\", row['Name'])\n",
    "\ttitle = m.group(1)\n",
    "\tif not np.isnan(row['Age']):\n",
    "\t\tcontinue\n",
    "\ttrain_data.at[index, 'Age'] = title_age_sum[title] / title_count[title]\n",
    "\n",
    "del both['Name']\n",
    "del both['Cabin']\n",
    "del both['Ticket']\n",
    "del both['Embarked']\n",
    "del both['Survived']\n",
    "\n",
    "mn = both.min()\n",
    "mx = both.max()\n",
    "\n",
    "del test_data['Name']\n",
    "del test_data['Cabin']\n",
    "del test_data['Ticket']\n",
    "del test_data['Embarked']\n",
    "\n",
    "y_train = train_data.loc[:, 'Survived']\n",
    "\n",
    "del train_data['Name']\n",
    "del train_data['Cabin']\n",
    "del train_data['Ticket']\n",
    "del train_data['Embarked']\n",
    "del train_data['Survived']\n",
    "\n",
    "test_data=(test_data-mn)/(mn+mx)\n",
    "train_data=(train_data-mn)/(mn+mx)\n",
    "\n",
    "X_train = train_data.loc[:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=10)\n",
    "\n",
    "hgb_cfl = ensemble.HistGradientBoostingClassifier(loss='auto', learning_rate=0.05)\n",
    "hgb_cfl.fit(X_train.values, y_train.values)\n",
    "print(hgb_cfl.score(X_test.values, y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372881355932204\n"
     ]
    }
   ],
   "source": [
    "gb_clf = ensemble.GradientBoostingClassifier(loss='exponential', learning_rate=0.5, n_estimators=100, subsample=1.)\n",
    "gb_clf.fit(X_train.values, y_train.values)\n",
    "print(gb_clf.score(X_test.values, y_test.values))\n",
    "y_pred_gb = gb_clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8101694915254237\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = neural_network.MLPClassifier(hidden_layer_sizes = (90,), alpha = 1e-20, max_iter = 200, activation='tanh', solver='lbfgs', learning_rate='constant') \n",
    "#no tanh maybe relu >> identity >> 10ogistic, no SGD, maybe adam\n",
    "#tanh + lbfgs ~.8 >> relu + adam\n",
    "mlp_clf.fit(X_train.values, y_train.values)\n",
    "print(mlp_clf.score(X_test.values, y_test.values))\n",
    "y_pred_mlp = mlp_clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8338983050847457\n"
     ]
    }
   ],
   "source": [
    "svm_clf = svm.SVC(kernel='linear')\n",
    "svm_clf.fit(X_train.values, y_train.values)\n",
    "print(svm_clf.score(X_test.values, y_test.values))\n",
    "y_pred_clf = svm_clf.predict(X_test.values)\n",
    "y_truth = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847457627118644\n"
     ]
    }
   ],
   "source": [
    "v_cfl = ensemble.VotingClassifier(estimators=[('mlp', mlp_clf), ('gb', gb_clf),('hgb', hgb_cfl)], voting='hard')\n",
    "v_cfl.fit(X_train.values, y_train.values)\n",
    "print(v_cfl.score(X_test.values, y_test.values))\n",
    "y_pred_v = v_cfl.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[169  22]\n",
      " [ 23  81]]\n",
      "\n",
      "True Negatives 169\n",
      "False Positives 22\n",
      "False Negatives 23\n",
      "True Positives 81\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_truth, y_pred_v).ravel()\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_truth, y_pred_v, labels=[0, 1]))\n",
    "print(\"\")\n",
    "print(\"True Negatives\", tn)\n",
    "print(\"False Positives\", fp)\n",
    "print(\"False Negatives\", fn)\n",
    "print(\"True Positives\", tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = ensemble.GradientBoostingClassifier(loss='exponential', learning_rate=0.5, n_estimators=100, subsample=1.)\n",
    "mlp_clf = neural_network.MLPClassifier(hidden_layer_sizes = (90,), alpha = 1e-20, max_iter = 200, activation='tanh', solver='lbfgs', learning_rate='constant') \n",
    "svm_clf = svm.SVC(kernel='linear')\n",
    "v_cfl = ensemble.VotingClassifier(estimators=[('mlp', mlp_clf), ('gb', gb_clf),('hgb', hgb_cfl)], voting='hard')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
